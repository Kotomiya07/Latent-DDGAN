{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc92f485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kotomiya/anaconda3/envs/ddgan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from datasets_prep.dataset import create_dataset\n",
    "#from diffusion import sample_from_model, sample_posterior, \\\n",
    "#    q_sample_pairs, get_time_schedule, \\\n",
    "#    Posterior_Coefficie\n",
    "#nts, Diffusion_Coefficients\n",
    "#from DWT_IDWT.DWT_IDWT_layer import DWT_2D, IDWT_2D\n",
    "#from pytorch_wavelets import DWTForward, DWTInverse\n",
    "from torch.multiprocessing import Process\n",
    "from utils import init_processes, copy_source, broadcast_params\n",
    "import yaml\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac15115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_config(config_path, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    config = OmegaConf.load(config_path)\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    #global_step = pl_sd[\"global_step\"]\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model = model.first_stage_model\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    del m\n",
    "    del u\n",
    "    del pl_sd\n",
    "    return model\n",
    "\n",
    "def grad_penalty_call(args, D_real, x_t):\n",
    "    grad_real = torch.autograd.grad(\n",
    "        outputs=D_real.sum(), inputs=x_t, create_graph=True\n",
    "    )[0]\n",
    "    grad_penalty = (\n",
    "        grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2\n",
    "    ).mean()\n",
    "\n",
    "    grad_penalty = args.r1_gamma / 2 * grad_penalty\n",
    "    grad_penalty.backward()\n",
    "\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3847cfee-ecfc-4249-afd8-7ec70e731249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "Working with z of shape (1, 4, 16, 16) = 1024 dimensions.\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoencoderKL(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down): ModuleList(\n",
       "      (0): Module(\n",
       "        (block): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (downsample): Downsample(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList(\n",
       "          (0-1): 2 x AttnBlock(\n",
       "            (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid): Module(\n",
       "      (block_1): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (attn_1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (block_2): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "    (conv_out): Conv2d(256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv_in): Conv2d(4, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (mid): Module(\n",
       "      (block_1): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (attn_1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (block_2): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (up): ModuleList(\n",
       "      (0): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "      )\n",
       "      (1): Module(\n",
       "        (block): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList(\n",
       "          (0-2): 3 x AttnBlock(\n",
       "            (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (upsample): Upsample(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (loss): LPIPSWithDiscriminator(\n",
       "    (perceptual_loss): LPIPS(\n",
       "      (scaling_layer): ScalingLayer()\n",
       "      (net): vgg16(\n",
       "        (slice1): Sequential(\n",
       "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU(inplace=True)\n",
       "        )\n",
       "        (slice2): Sequential(\n",
       "          (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (6): ReLU(inplace=True)\n",
       "          (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (8): ReLU(inplace=True)\n",
       "        )\n",
       "        (slice3): Sequential(\n",
       "          (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (11): ReLU(inplace=True)\n",
       "          (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (13): ReLU(inplace=True)\n",
       "          (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (15): ReLU(inplace=True)\n",
       "        )\n",
       "        (slice4): Sequential(\n",
       "          (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (18): ReLU(inplace=True)\n",
       "          (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (20): ReLU(inplace=True)\n",
       "          (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (22): ReLU(inplace=True)\n",
       "        )\n",
       "        (slice5): Sequential(\n",
       "          (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (25): ReLU(inplace=True)\n",
       "          (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (27): ReLU(inplace=True)\n",
       "          (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (29): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (lin0): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (lin1): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (lin2): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (lin3): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (lin4): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (discriminator): NLayerDiscriminator(\n",
       "      (main): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ldm.util import instantiate_from_config\n",
    "config_path = \"./autoencoder/config/cifar10_16x16x4.yaml\"\n",
    "ckpt_path = \"./autoencoder/weight/16x16x4_551.ckpt\"\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "AutoEncoder = instantiate_from_config(config['model'])\n",
    "\n",
    "\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "AutoEncoder.load_state_dict(checkpoint['state_dict'])\n",
    "AutoEncoder.eval()\n",
    "AutoEncoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c8076c-c0c9-4ffa-98f4-15967608972a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoencoderKL(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down): ModuleList(\n",
       "      (0): Module(\n",
       "        (block): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (downsample): Downsample(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (downsample): Downsample(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (2): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "      )\n",
       "    )\n",
       "    (mid): Module(\n",
       "      (block_1): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (attn_1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (block_2): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "    (conv_out): Conv2d(512, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv_in): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (mid): Module(\n",
       "      (block_1): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (attn_1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (block_2): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (up): ModuleList(\n",
       "      (0): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "      )\n",
       "      (1): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (upsample): Upsample(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): Module(\n",
       "        (block): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (upsample): Upsample(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (loss): LPIPSWithDiscriminator(\n",
       "    (perceptual_loss): LPIPS(\n",
       "      (scaling_layer): ScalingLayer()\n",
       "      (net): vgg16(\n",
       "        (slice1): Sequential(\n",
       "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU(inplace=True)\n",
       "        )\n",
       "        (slice2): Sequential(\n",
       "          (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (6): ReLU(inplace=True)\n",
       "          (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (8): ReLU(inplace=True)\n",
       "        )\n",
       "        (slice3): Sequential(\n",
       "          (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (11): ReLU(inplace=True)\n",
       "          (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (13): ReLU(inplace=True)\n",
       "          (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (15): ReLU(inplace=True)\n",
       "        )\n",
       "        (slice4): Sequential(\n",
       "          (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (18): ReLU(inplace=True)\n",
       "          (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (20): ReLU(inplace=True)\n",
       "          (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (22): ReLU(inplace=True)\n",
       "        )\n",
       "        (slice5): Sequential(\n",
       "          (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (25): ReLU(inplace=True)\n",
       "          (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (27): ReLU(inplace=True)\n",
       "          (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (29): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (lin0): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (lin1): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (lin2): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (lin3): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (lin4): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (discriminator): NLayerDiscriminator(\n",
       "      (main): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (quant_conv): Conv2d(6, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (post_quant_conv): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ldm.util import instantiate_from_config\n",
    "config_path = \"./autoencoder/config/COCO_config.yaml\"\n",
    "ckpt_path = \"./autoencoder/weight/kl-f4.ckpt\"\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "AutoEncoder = instantiate_from_config(config['model'])\n",
    "\n",
    "\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "AutoEncoder.load_state_dict(checkpoint['state_dict'])\n",
    "AutoEncoder.eval()\n",
    "AutoEncoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2396ce6d-4138-49ed-aa9d-95b5fc3d0792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.66s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import CIFAR10, STL10\n",
    "\n",
    "from datasets_prep.coco import CustomCocoCaptions\n",
    "size = 128\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.transforms.CenterCrop(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "dataset = CustomCocoCaptions(\n",
    "    name=\"train2017\",\n",
    "    root=os.path.join(\"./data/coco\", \"train2017\"),\n",
    "    annFile=os.path.join(\n",
    "    \"./data/coco\", 'annotations', 'captions_train2017.json'), \n",
    "    transform=train_transform)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2,\n",
    "                                          pin_memory=True,\n",
    "                                          #sampler=train_sampler,\n",
    "                                          drop_last=True\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42869ed2-c3cc-45db-886d-c2021032dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = iter(data_loader)\n",
    "x, y = next(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a228d5a7-2ce6-4136-baa0-edde2a110414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ldm.modules.distributions.distributions.DiagonalGaussianDistribution'>\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "scale_factor = 1.0\n",
    "x0 = x.to(device, non_blocking=True)\n",
    "with torch.no_grad():\n",
    "    posterior = AutoEncoder.encode(x0)\n",
    "    print(type(posterior))\n",
    "    real_data = posterior.sample().detach()\n",
    "#print(\"MIN:{}, MAX:{}\".format(real_data.min(), real_data.max()))\n",
    "real_data = real_data / scale_factor #300.0  # [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb020d42-b504-4956-bfd7-9de347115c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data *= scale_factor #300\n",
    "with torch.no_grad():\n",
    "    real_data = AutoEncoder.decode(real_data)\n",
    "\n",
    "real_data = (torch.clamp(real_data, -1, 1) + 1) / 2  # 0-1 \n",
    "\n",
    "torchvision.utils.save_image(\n",
    "    real_data, os.path.join(\"./\", 'real_data.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ecd5c314-235e-4588-86a0-726e897e8957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = CIFAR10(\"./data/cifar-10\", train=True, transform=transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]), download=True)\n",
    "data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                          batch_size=1,           \n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "059ee6d2-be12-4671-8262-a469d9330751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASTER_PORT=6038\n",
      "The work dir is:  /mnt/box/MyFolder/project/Latent-DDGAN\n",
      "cifar10 train 1\n",
      "==> Training LDDGAN\n",
      "starting in debug mode\n",
      "git root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/mnt/box/MyFolder/project/Latent-DDGAN'\n",
      "To add an exception for this directory, call:\n",
      "\n",
      "\tgit config --global --add safe.directory /mnt/box/MyFolder/project/Latent-DDGAN'\n",
      "git root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/mnt/box/MyFolder/project/Latent-DDGAN'\n",
      "To add an exception for this directory, call:\n",
      "\n",
      "\tgit config --global --add safe.directory /mnt/box/MyFolder/project/Latent-DDGAN'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkotomiya07\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/mnt/box/MyFolder/project/Latent-DDGAN/wandb/run-20240724_135447-o9fw1hwt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdriven-hill-58\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kotomiya07/LDGAN\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kotomiya07/LDGAN/runs/o9fw1hwt\u001b[0m\n",
      "Files already downloaded and verified\n",
      "GEN: <class 'score_sde.models.ncsnpp_generator_adagn.NCSNpp'>, DISC: [<class 'score_sde.models.discriminator.Discriminator_small'>, <class 'score_sde.models.discriminator.Discriminator_large'>]\n",
      "2024-07-24 13:54:55.457676: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-24 13:54:55.476835: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-24 13:54:55.476879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-24 13:54:55.477643: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-24 13:54:55.480865: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-24 13:54:56.109358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "/home/kotomiya/anaconda3/envs/ddgan/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/kotomiya/anaconda3/envs/ddgan/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/box/MyFolder/project/Latent-DDGAN/train_ldgan.py\", line 541, in <module>\n",
      "    init_processes(0, size, train, args)\n",
      "  File \"/mnt/box/MyFolder/project/Latent-DDGAN/utils.py\", line 25, in init_processes\n",
      "    fn(rank, gpu, args)\n",
      "  File \"/mnt/box/MyFolder/project/Latent-DDGAN/train_ldgan.py\", line 170, in train\n",
      "    netG.load_state_dict(checkpoint['netG_dict'])\n",
      "  File \"/home/kotomiya/anaconda3/envs/ddgan/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2153, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for DistributedDataParallel:\n",
      "\tsize mismatch for module.all_modules.2.weight: copying a param with shape torch.Size([128, 4, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 3, 3, 3]).\n",
      "\tsize mismatch for module.all_modules.6.Conv2d_0.weight: copying a param with shape torch.Size([128, 4, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 3, 3, 3]).\n",
      "\tsize mismatch for module.all_modules.28.weight: copying a param with shape torch.Size([4, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([3, 128, 3, 3]).\n",
      "\tsize mismatch for module.all_modules.28.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([3]).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mdriven-hill-58\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/kotomiya07/LDGAN/runs/o9fw1hwt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/kotomiya07/LDGAN\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240724_135447-o9fw1hwt/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
     ]
    }
   ],
   "source": [
    "!bash lab_run.sh cifar10 train 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6a70b7-f8ee-4a5c-ab5e-6cbf0975030c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
