{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc92f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from datasets_prep.dataset import create_dataset\n",
    "from diffusion import sample_from_model, sample_posterior, \\\n",
    "    q_sample_pairs, get_time_schedule, \\\n",
    "    Posterior_Coefficients, Diffusion_Coefficients\n",
    "#from DWT_IDWT.DWT_IDWT_layer import DWT_2D, IDWT_2D\n",
    "#from pytorch_wavelets import DWTForward, DWTInverse\n",
    "from torch.multiprocessing import Process\n",
    "from utils import init_processes, copy_source, broadcast_params\n",
    "import yaml\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from omegaconf import OmegaConf\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ac15115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_config(config_path, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    config = OmegaConf.load(config_path)\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    #global_step = pl_sd[\"global_step\"]\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model = model.first_stage_model\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    del m\n",
    "    del u\n",
    "    del pl_sd\n",
    "    return model\n",
    "\n",
    "def grad_penalty_call(args, D_real, x_t):\n",
    "    grad_real = torch.autograd.grad(\n",
    "        outputs=D_real.sum(), inputs=x_t, create_graph=True\n",
    "    )[0]\n",
    "    grad_penalty = (\n",
    "        grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2\n",
    "    ).mean()\n",
    "\n",
    "    grad_penalty = args.r1_gamma / 2 * grad_penalty\n",
    "    grad_penalty.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b89feeaa-6994-422a-9733-73739e34608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def train(rank, gpu, args):\n",
    "    from EMA import EMA\n",
    "    from score_sde.models.discriminator import Discriminator_large, Discriminator_small\n",
    "    from score_sde.models.ncsnpp_generator_adagn import NCSNpp, WaveletNCSNpp\n",
    "\n",
    "    torch.manual_seed(args.seed + rank)\n",
    "    torch.cuda.manual_seed(args.seed + rank)\n",
    "    torch.cuda.manual_seed_all(args.seed + rank)\n",
    "    device = torch.device('cuda:{}'.format(gpu))\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "\n",
    "    nz = args.nz  # latent dimension\n",
    "\n",
    "    dataset = create_dataset(args)\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset,\n",
    "                                                                    num_replicas=args.world_size,\n",
    "                                                                    rank=rank)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=args.num_workers,\n",
    "                                              pin_memory=True,\n",
    "                                              sampler=train_sampler,\n",
    "                                              drop_last=True)\n",
    "    args.ori_image_size = args.image_size\n",
    "    args.image_size = args.current_resolution\n",
    "    G_NET_ZOO = {\"normal\": NCSNpp, \"wavelet\": WaveletNCSNpp}\n",
    "    gen_net = G_NET_ZOO[args.net_type]\n",
    "    disc_net = [Discriminator_small, Discriminator_large]\n",
    "    print(\"GEN: {}, DISC: {}\".format(gen_net, disc_net))\n",
    "    netG = gen_net(args).to(device)\n",
    "\n",
    "    if args.dataset in ['cifar10', 'stl10']:\n",
    "        netD = disc_net[0](nc=2 * args.num_channels, ngf=args.ngf,\n",
    "                           t_emb_dim=args.t_emb_dim,\n",
    "                           act=nn.LeakyReLU(0.2), num_layers=args.num_disc_layers).to(device)\n",
    "    else:\n",
    "        netD = disc_net[1](nc=2 * args.num_channels, ngf=args.ngf,\n",
    "                           t_emb_dim=args.t_emb_dim,\n",
    "                           act=nn.LeakyReLU(0.2), num_layers=args.num_disc_layers).to(device)\n",
    "\n",
    "    broadcast_params(netG.parameters())\n",
    "    broadcast_params(netD.parameters())\n",
    "\n",
    "    optimizerD = optim.Adam(filter(lambda p: p.requires_grad, netD.parameters(\n",
    "    )), lr=args.lr_d, betas=(args.beta1, args.beta2))\n",
    "    optimizerG = optim.Adam(filter(lambda p: p.requires_grad, netG.parameters(\n",
    "    )), lr=args.lr_g, betas=(args.beta1, args.beta2))\n",
    "\n",
    "    if args.use_ema:\n",
    "        optimizerG = EMA(optimizerG, ema_decay=args.ema_decay)\n",
    "\n",
    "    schedulerG = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizerG, args.num_epoch, eta_min=1e-5)\n",
    "    schedulerD = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizerD, args.num_epoch, eta_min=1e-5)\n",
    "\n",
    "    # ddp\n",
    "    netG = nn.parallel.DistributedDataParallel(\n",
    "        netG, device_ids=[gpu], find_unused_parameters=True)\n",
    "    netD = nn.parallel.DistributedDataParallel(netD, device_ids=[gpu])\n",
    "\n",
    "    \"\"\"############### DELETE TO AVOID ERROR ###############\"\"\"\n",
    "    # Wavelet Pooling\n",
    "    #if not args.use_pytorch_wavelet:\n",
    "    #    dwt = DWT_2D(\"haar\")\n",
    "    #    iwt = IDWT_2D(\"haar\")\n",
    "    #else:\n",
    "    #    dwt = DWTForward(J=1, mode='zero', wave='haar').cuda()\n",
    "    #    iwt = DWTInverse(mode='zero', wave='haar').cuda()\n",
    "        \n",
    "    \n",
    "    #load encoder and decoder\n",
    "    config_path = args.AutoEncoder_config \n",
    "    ckpt_path = args.AutoEncoder_ckpt \n",
    "    \n",
    "    if args.dataset in ['cifar10', 'stl10']:\n",
    "\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        \n",
    "        AutoEncoder = instantiate_from_config(config['model'])\n",
    "        \n",
    "\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        AutoEncoder.load_state_dict(checkpoint['state_dict'])\n",
    "        AutoEncoder.eval()\n",
    "        AutoEncoder.to(device)\n",
    "    \n",
    "    else:\n",
    "        AutoEncoder = load_model_from_config(config_path, ckpt_path)\n",
    "    \"\"\"############### END DELETING ###############\"\"\"\n",
    "    \n",
    "    num_levels = int(np.log2(args.ori_image_size // args.current_resolution))\n",
    "\n",
    "    exp = args.exp\n",
    "    parent_dir = \"./saved_info/{}\".format(args.dataset)\n",
    "\n",
    "    exp_path = os.path.join(parent_dir, exp)\n",
    "    if rank == 0:\n",
    "        if not os.path.exists(exp_path):\n",
    "            os.makedirs(exp_path)\n",
    "            copy_source(__file__, exp_path)\n",
    "            shutil.copytree('score_sde/models', os.path.join(exp_path, 'score_sde/models'))\n",
    "\n",
    "    coeff = Diffusion_Coefficients(args, device)\n",
    "    pos_coeff = Posterior_Coefficients(args, device)\n",
    "    T = get_time_schedule(args, device)\n",
    "\n",
    "    if args.resume or os.path.exists(os.path.join(exp_path, 'content.pth')):\n",
    "        checkpoint_file = os.path.join(exp_path, 'content.pth')\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "        init_epoch = checkpoint['epoch']\n",
    "        epoch = init_epoch\n",
    "        # load G\n",
    "        netG.load_state_dict(checkpoint['netG_dict'])\n",
    "        #optimizerG.load_state_dict(checkpoint['optimizerG'])\n",
    "        schedulerG.load_state_dict(checkpoint['schedulerG'])\n",
    "        # load D\n",
    "        netD.load_state_dict(checkpoint['netD_dict'])\n",
    "        #optimizerD.load_state_dict(checkpoint['optimizerD'])\n",
    "        schedulerD.load_state_dict(checkpoint['schedulerD'])\n",
    "\n",
    "        global_step = checkpoint['global_step']\n",
    "        print(\"=> loaded checkpoint (epoch {})\"\n",
    "              .format(checkpoint['epoch']))\n",
    "    else:\n",
    "        global_step, epoch, init_epoch = 0, 0, 0\n",
    "\n",
    "    '''Sigmoid learning parameter'''\n",
    "    gamma = 6\n",
    "    beta = np.linspace(-gamma, gamma, args.num_epoch+1)\n",
    "    alpha = 1 - 1 / (1+np.exp(-beta))\n",
    "\n",
    "    for epoch in range(init_epoch, args.num_epoch + 1):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        \n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        for iteration, (x, y) in enumerate(data_loader):\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = True\n",
    "            netD.zero_grad()\n",
    "\n",
    "            for p in netG.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # sample from p(x_0)\n",
    "            x0 = x.to(device, non_blocking=True)\n",
    "\n",
    "            \"\"\"################# Change here: Encoder #################\"\"\"\n",
    "            with torch.no_grad():\n",
    "                posterior = AutoEncoder.encode(x0)\n",
    "                real_data = posterior.sample().detach()\n",
    "            #print(\"MIN:{}, MAX:{}\".format(real_data.min(), real_data.max()))\n",
    "            real_data = real_data / args.scale_factor #300.0  # [-1, 1]\n",
    "            \n",
    "            \n",
    "            #assert -1 <= real_data.min() < 0\n",
    "            #assert 0 < real_data.max() <= 1\n",
    "            \"\"\"################# End change: Encoder #################\"\"\"\n",
    "            # sample t\n",
    "            t = torch.randint(0, args.num_timesteps,\n",
    "                              (real_data.size(0),), device=device)\n",
    "\n",
    "            x_t, x_tp1 = q_sample_pairs(coeff, real_data, t)\n",
    "            x_t.requires_grad = True\n",
    "\n",
    "            # train with real\n",
    "            D_real = netD(x_t, t, x_tp1.detach()).view(-1)\n",
    "            errD_real = F.softplus(-D_real).mean()\n",
    "\n",
    "            errD_real.backward(retain_graph=True)\n",
    "\n",
    "            if args.lazy_reg is None:\n",
    "                grad_penalty_call(args, D_real, x_t)\n",
    "            else:\n",
    "                if global_step % args.lazy_reg == 0:\n",
    "                    grad_penalty_call(args, D_real, x_t)\n",
    "\n",
    "            # train with fake\n",
    "            latent_z = torch.randn(batch_size, nz, device=device)\n",
    "            x_0_predict = netG(x_tp1.detach(), t, latent_z)\n",
    "            x_pos_sample = sample_posterior(pos_coeff, x_0_predict, x_tp1, t)\n",
    "\n",
    "            output = netD(x_pos_sample, t, x_tp1.detach()).view(-1)\n",
    "            errD_fake = F.softplus(output).mean()\n",
    "\n",
    "            errD_fake.backward()\n",
    "\n",
    "            errD = errD_real + errD_fake\n",
    "            # Update D\n",
    "            optimizerD.step()\n",
    "\n",
    "            # update G\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            for p in netG.parameters():\n",
    "                p.requires_grad = True\n",
    "            netG.zero_grad()\n",
    "\n",
    "            t = torch.randint(0, args.num_timesteps,\n",
    "                              (real_data.size(0),), device=device)\n",
    "            x_t, x_tp1 = q_sample_pairs(coeff, real_data, t)\n",
    "\n",
    "            latent_z = torch.randn(batch_size, nz, device=device)\n",
    "            x_0_predict = netG(x_tp1.detach(), t, latent_z)\n",
    "            x_pos_sample = sample_posterior(pos_coeff, x_0_predict, x_tp1, t)\n",
    "\n",
    "            output = netD(x_pos_sample, t, x_tp1.detach()).view(-1)\n",
    "            errG = F.softplus(-output).mean()\n",
    "\n",
    "            # reconstructior loss\n",
    "            if args.sigmoid_learning and args.rec_loss:\n",
    "                ######alpha\n",
    "                rec_loss = F.l1_loss(x_0_predict, real_data)\n",
    "                errG = errG + alpha[epoch]*rec_loss\n",
    "\n",
    "            elif args.rec_loss and not args.sigmoid_learning:\n",
    "                rec_loss = F.l1_loss(x_0_predict, real_data)\n",
    "                errG = errG + rec_loss\n",
    "            \n",
    "\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            global_step += 1\n",
    "            if iteration % 100 == 0:\n",
    "                if rank == 0:\n",
    "                    end.record()\n",
    "                    torch.cuda.synchronize()\n",
    "                    elapsed_time = start.elapsed_time(end)\n",
    "                    if args.sigmoid_learning:\n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}, alpha: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item(), alpha[epoch]))\n",
    "                    elif args.rec_loss:\n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}, rec_loss: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item(), rec_loss.item()))\n",
    "                    else:   \n",
    "                        print('epoch {} iteration{}, G Loss: {}, D Loss: {}'.format(\n",
    "                            epoch, iteration, errG.item(), errD.item()))\n",
    "                    wandb.log({\"iteration:\": iteration, \"G_loss\": errG.item(), \"D_loss\": errD.item(), \"alpha\": alpha[epoch], \"elapsed_time\": elapsed_time / 1000})\n",
    "\n",
    "        if not args.no_lr_decay:\n",
    "\n",
    "            schedulerG.step()\n",
    "            schedulerD.step()\n",
    "\n",
    "        if rank == 0:\n",
    "            ########################################\n",
    "            x_t_1 = torch.randn_like(posterior.sample())\n",
    "            fake_sample = sample_from_model(\n",
    "                pos_coeff, netG, args.num_timesteps, x_t_1, T, args)\n",
    "\n",
    "            \"\"\"############## CHANGE HERE: DECODER ##############\"\"\"\n",
    "            fake_sample *= args.scale_factor #300\n",
    "            real_data *= args.scale_factor #300\n",
    "            with torch.no_grad():\n",
    "                fake_sample = AutoEncoder.decode(fake_sample)\n",
    "                real_data = AutoEncoder.decode(real_data)\n",
    "            \n",
    "            fake_sample = (torch.clamp(fake_sample, -1, 1) + 1) / 2  # 0-1\n",
    "            real_data = (torch.clamp(real_data, -1, 1) + 1) / 2  # 0-1 \n",
    "            \n",
    "            \"\"\"############## END HERE: DECODER ##############\"\"\"\n",
    "\n",
    "            torchvision.utils.save_image(fake_sample, os.path.join(\n",
    "                exp_path, 'sample_discrete_epoch_{}.png'.format(epoch)))\n",
    "            torchvision.utils.save_image(\n",
    "                real_data, os.path.join(exp_path, 'real_data.png'))\n",
    "\n",
    "            if args.save_content:\n",
    "                if epoch % args.save_content_every == 0:\n",
    "                    print('Saving content.')\n",
    "                    content = {'epoch': epoch + 1, 'global_step': global_step, 'args': args,\n",
    "                               'netG_dict': netG.state_dict(), 'optimizerG': optimizerG.state_dict(),\n",
    "                               'schedulerG': schedulerG.state_dict(), 'netD_dict': netD.state_dict(),\n",
    "                               'optimizerD': optimizerD.state_dict(), 'schedulerD': schedulerD.state_dict()}\n",
    "                    torch.save(content, os.path.join(exp_path, 'content.pth'))\n",
    "\n",
    "            if epoch % args.save_ckpt_every == 0:\n",
    "                if args.use_ema:\n",
    "                    optimizerG.swap_parameters_with_ema(\n",
    "                        store_params_in_ema=True)\n",
    "\n",
    "                torch.save(netG.state_dict(), os.path.join(\n",
    "                    exp_path, 'netG_{}.pth'.format(epoch)))\n",
    "                if args.use_ema:\n",
    "                    optimizerG.swap_parameters_with_ema(\n",
    "                        store_params_in_ema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14908090-7e68-4010-b192-6939cdbea8d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ArgumentParser.__init__() got an unexpected keyword argument 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43margparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mArgumentParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--seed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m      3\u001b[0m                     help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed used for initialization\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--resume\u001b[39m\u001b[38;5;124m'\u001b[39m, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore_true\u001b[39m\u001b[38;5;124m'\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: ArgumentParser.__init__() got an unexpected keyword argument 'args'"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(args=[])\n",
    "parser.add_argument('--seed', type=int, default=1024,\n",
    "                    help='seed used for initialization')\n",
    "\n",
    "parser.add_argument('--resume', action='store_true', default=False)\n",
    "\n",
    "parser.add_argument('--image_size', type=int, default=32,\n",
    "                    help='size of image')\n",
    "parser.add_argument('--num_channels', type=int, default=12,\n",
    "                    help='channel of wavelet subbands')\n",
    "parser.add_argument('--centered', action='store_false', default=True,\n",
    "                    help='-1,1 scale')\n",
    "parser.add_argument('--use_geometric', action='store_true', default=False)\n",
    "parser.add_argument('--beta_min', type=float, default=0.1,\n",
    "                    help='beta_min for diffusion')\n",
    "parser.add_argument('--beta_max', type=float, default=20.,\n",
    "                    help='beta_max for diffusion')\n",
    "\n",
    "parser.add_argument('--patch_size', type=int, default=1,\n",
    "                    help='Patchify image into non-overlapped patches')\n",
    "parser.add_argument('--num_channels_dae', type=int, default=128,\n",
    "                    help='number of initial channels in denosing model')\n",
    "parser.add_argument('--n_mlp', type=int, default=3,\n",
    "                    help='number of mlp layers for z')\n",
    "parser.add_argument('--ch_mult', nargs='+', type=int,\n",
    "                    help='channel multiplier')\n",
    "parser.add_argument('--num_res_blocks', type=int, default=2,\n",
    "                    help='number of resnet blocks per scale')\n",
    "parser.add_argument('--attn_resolutions', default=(16,), nargs='+', type=int,\n",
    "                    help='resolution of applying attention')\n",
    "parser.add_argument('--dropout', type=float, default=0.,\n",
    "                    help='drop-out rate')\n",
    "parser.add_argument('--resamp_with_conv', action='store_false', default=True,\n",
    "                    help='always up/down sampling with conv')\n",
    "parser.add_argument('--conditional', action='store_false', default=True,\n",
    "                    help='noise conditional')\n",
    "parser.add_argument('--fir', action='store_false', default=True,\n",
    "                    help='FIR')\n",
    "parser.add_argument('--fir_kernel', default=[1, 3, 3, 1],\n",
    "                    help='FIR kernel')\n",
    "parser.add_argument('--skip_rescale', action='store_false', default=True,\n",
    "                    help='skip rescale')\n",
    "parser.add_argument('--resblock_type', default='biggan',\n",
    "                    help='tyle of resnet block, choice in biggan and ddpm')\n",
    "parser.add_argument('--progressive', type=str, default='none', choices=['none', 'output_skip', 'residual'],\n",
    "                    help='progressive type for output')\n",
    "parser.add_argument('--progressive_input', type=str, default='residual', choices=['none', 'input_skip', 'residual'],\n",
    "                    help='progressive type for input')\n",
    "parser.add_argument('--progressive_combine', type=str, default='sum', choices=['sum', 'cat'],\n",
    "                    help='progressive combine method.')\n",
    "\n",
    "parser.add_argument('--embedding_type', type=str, default='positional', choices=['positional', 'fourier'],\n",
    "                    help='type of time embedding')\n",
    "parser.add_argument('--fourier_scale', type=float, default=16.,\n",
    "                    help='scale of fourier transform')\n",
    "parser.add_argument('--not_use_tanh', action='store_true', default=False)\n",
    "\n",
    "# generator and training\n",
    "parser.add_argument(\n",
    "    '--exp', default='experiment_cifar_default', help='name of experiment')\n",
    "parser.add_argument('--dataset', default='cifar10', help='name of dataset')\n",
    "parser.add_argument('--datadir', default='./data')\n",
    "parser.add_argument('--nz', type=int, default=100)\n",
    "parser.add_argument('--num_timesteps', type=int, default=4)\n",
    "\n",
    "parser.add_argument('--z_emb_dim', type=int, default=256)\n",
    "parser.add_argument('--t_emb_dim', type=int, default=256)\n",
    "parser.add_argument('--batch_size', type=int,\n",
    "                    default=128, help='input batch size')\n",
    "parser.add_argument('--num_epoch', type=int, default=1200)\n",
    "parser.add_argument('--ngf', type=int, default=64)\n",
    "\n",
    "parser.add_argument('--lr_g', type=float,\n",
    "                    default=1.5e-4, help='learning rate g')\n",
    "parser.add_argument('--lr_d', type=float, default=1e-4,\n",
    "                    help='learning rate d')\n",
    "parser.add_argument('--beta1', type=float, default=0.5,\n",
    "                    help='beta1 for adam')\n",
    "parser.add_argument('--beta2', type=float, default=0.9,\n",
    "                    help='beta2 for adam')\n",
    "parser.add_argument('--no_lr_decay', action='store_true', default=False)\n",
    "\n",
    "parser.add_argument('--use_ema', action='store_true', default=False,\n",
    "                    help='use EMA or not')\n",
    "parser.add_argument('--ema_decay', type=float,\n",
    "                    default=0.9999, help='decay rate for EMA')\n",
    "\n",
    "parser.add_argument('--r1_gamma', type=float,\n",
    "                    default=0.05, help='coef for r1 reg')\n",
    "parser.add_argument('--lazy_reg', type=int, default=None,\n",
    "                    help='lazy regulariation.')\n",
    "\n",
    "# wavelet GAN\n",
    "parser.add_argument(\"--current_resolution\", type=int, default=256)\n",
    "parser.add_argument(\"--use_pytorch_wavelet\", action=\"store_true\")\n",
    "parser.add_argument(\"--rec_loss\", action=\"store_true\")\n",
    "parser.add_argument(\"--net_type\", default=\"normal\")\n",
    "parser.add_argument(\"--num_disc_layers\", default=6, type=int)\n",
    "parser.add_argument(\"--no_use_fbn\", action=\"store_true\")\n",
    "parser.add_argument(\"--no_use_freq\", action=\"store_true\")\n",
    "parser.add_argument(\"--no_use_residual\", action=\"store_true\")\n",
    "\n",
    "parser.add_argument('--save_content', action='store_true', default=False)\n",
    "parser.add_argument('--save_content_every', type=int, default=50,\n",
    "                    help='save content for resuming every x epochs')\n",
    "parser.add_argument('--save_ckpt_every', type=int,\n",
    "                    default=25, help='save ckpt every x epochs')\n",
    "\n",
    "# ddp\n",
    "parser.add_argument('--num_proc_node', type=int, default=1,\n",
    "                    help='The number of nodes in multi node env.')\n",
    "parser.add_argument('--num_process_per_node', type=int, default=1,\n",
    "                    help='number of gpus')\n",
    "parser.add_argument('--node_rank', type=int, default=0,\n",
    "                    help='The index of node.')\n",
    "parser.add_argument('--local_rank', type=int, default=0,\n",
    "                    help='rank of process in the node')\n",
    "parser.add_argument('--master_address', type=str, default='127.0.0.1',\n",
    "                    help='address for master')\n",
    "parser.add_argument('--master_port', type=str, default='6002',\n",
    "                    help='port for master')\n",
    "parser.add_argument('--num_workers', type=int, default=4,\n",
    "                    help='num_workers')\n",
    "\n",
    "##### My parameter #####\n",
    "parser.add_argument('--scale_factor', type=float, default=16.,\n",
    "                    help='scale of Encoder output')\n",
    "parser.add_argument(\n",
    "    '--AutoEncoder_config', default='./autoencoder/config/autoencoder_kl_f2_16x16x4_Cifar10_big.yaml', help='path of config file for AntoEncoder')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--AutoEncoder_ckpt', default='./autoencoder/weight/last_big.ckpt', help='path of weight for AntoEncoder')\n",
    "\n",
    "parser.add_argument(\"--sigmoid_learning\", action=\"store_true\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.world_size = args.num_proc_node * args.num_process_per_node\n",
    "size = args.num_process_per_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18057f37-46a2-4b9c-9c37-a316f2300526",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m      2\u001b[0m             project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatent-diffusion-gan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m             config\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m----> 4\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mdataset,\n\u001b[1;32m      5\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mimage_size,\n\u001b[1;32m      6\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mnum_channels,\n\u001b[1;32m      7\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mnum_timesteps,\n\u001b[1;32m      8\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnz\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mnz,\n\u001b[1;32m      9\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mnum_epoch,\n\u001b[1;32m     10\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mngf\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mngf,\n\u001b[1;32m     11\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_g\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mlr_g,\n\u001b[1;32m     12\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_d\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mlr_d,\n\u001b[1;32m     13\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m     14\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr1_gamma\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mr1_gamma,\n\u001b[1;32m     15\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlazy_reg\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mlazy_reg,\n\u001b[1;32m     16\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_ema\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39muse_ema,\n\u001b[1;32m     17\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mema_decay,\n\u001b[1;32m     18\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_lr_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mno_lr_decay,\n\u001b[1;32m     19\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_pytorch_wavelet\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39muse_pytorch_wavelet,\n\u001b[1;32m     20\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mrec_loss,\n\u001b[1;32m     21\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnet_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mnet_type,\n\u001b[1;32m     22\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_disc_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mnum_disc_layers,\n\u001b[1;32m     23\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_use_fbn\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mno_use_fbn,\n\u001b[1;32m     24\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_use_freq\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mno_use_freq,\n\u001b[1;32m     25\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_use_residual\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mno_use_residual,\n\u001b[1;32m     26\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mscale_factor,\n\u001b[1;32m     27\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoEncoder_config\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mAutoEncoder_config,\n\u001b[1;32m     28\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoEncoder_ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mAutoEncoder_ckpt,\n\u001b[1;32m     29\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmoid_learning\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39msigmoid_learning,\n\u001b[1;32m     30\u001b[0m             }\n\u001b[1;32m     31\u001b[0m         )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "            project=\"latent-diffusion-gan\",\n",
    "            config={\n",
    "                \"dataset\": args.dataset,\n",
    "                \"image_size\": args.image_size,\n",
    "                \"channels\": args.num_channels,\n",
    "                \"timesteps\": args.num_timesteps,\n",
    "                \"nz\": args.nz,\n",
    "                \"epochs\": args.num_epoch,\n",
    "                \"ngf\": args.ngf,\n",
    "                \"lr_g\": args.lr_g,\n",
    "                \"lr_d\": args.lr_d,\n",
    "                \"batch_size\": args.batch_size,\n",
    "                \"r1_gamma\": args.r1_gamma,\n",
    "                \"lazy_reg\": args.lazy_reg,\n",
    "                \"use_ema\": args.use_ema,\n",
    "                \"ema_decay\": args.ema_decay,\n",
    "                \"no_lr_decay\": args.no_lr_decay,\n",
    "                \"use_pytorch_wavelet\": args.use_pytorch_wavelet,\n",
    "                \"rec_loss\": args.rec_loss,\n",
    "                \"net_type\": args.net_type,\n",
    "                \"num_disc_layers\": args.num_disc_layers,\n",
    "                \"no_use_fbn\": args.no_use_fbn,\n",
    "                \"no_use_freq\": args.no_use_freq,\n",
    "                \"no_use_residual\": args.no_use_residual,\n",
    "                \"scale_factor\": args.scale_factor,\n",
    "                \"AutoEncoder_config\": args.AutoEncoder_config,\n",
    "                \"AutoEncoder_ckpt\": args.AutoEncoder_ckpt,\n",
    "                \"sigmoid_learning\": args.sigmoid_learning,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4715c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_processes(0, size, train, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8e07f695-44e5-4cce-a66a-fb5bf83269d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.78s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#@title dataloader test\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import CIFAR10, STL10, CocoCaptions\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.transforms.CenterCrop(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "dataset = CocoCaptions(\n",
    "    root=os.path.join(\"./data/coco\", \"train2017\"), \n",
    "    annFile=os.path.join(\n",
    "    \"./data/coco\", 'annotations', 'captions_train2017.json'), \n",
    "    transform=train_transform)\n",
    "#train_sampler = torch.utils.data.distributed.DistributedSampler(dataset,num_replicas=1,rank=0)\n",
    "data_loader = torch.utils.data.DataLoader(dataset,\\\n",
    "                                        batch_size=8,\\\n",
    "                                        shuffle=False,\\\n",
    "                                        num_workers=2,\\\n",
    "                                        pin_memory=True,\\\n",
    "                                        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1e00e789-7096-44d6-98a4-162853687f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 256, 256]) <class 'torch.Tensor'>\n",
      "[['Closeup of bins of food that include broccoli and bread.', 'A giraffe eating food from the top of the tree.', 'A flower vase is sitting on a porch stand.', 'A zebra grazing on lush green grass in a field.', 'Woman in swim suit holding parasol on sunny day.', 'This wire metal rack holds several pairs of shoes and sandals', 'A couple of men riding horses on top of a green field.', 'They are brave for riding in the jungle on those elephants.'], ['A meal is presented in brightly colored plastic trays.', 'A giraffe standing up nearby a tree ', 'White vase with different colored flowers sitting inside of it. ', 'Zebra reaching its head down to ground where grass is. ', 'A woman posing for the camera, holding a pink, open umbrella and wearing a bright, floral, ruched bathing suit, by a life guard stand with lake, green trees, and a blue sky with a few clouds behind.', 'A dog sleeping on a show rack in the shoes.', 'two horses and their riders on some grass', 'SOME PEOPLE IN THE WOODS RIDING TWO ELEPHANTS'], ['there are containers filled with different kinds of foods', 'A giraffe mother with its baby in the forest.', 'a white vase with many flowers on a stage', 'The zebra is eating grass in the sun.', 'A woman in a floral swimsuit holds a pink umbrella.', 'Various slides and other footwear rest in a metal basket outdoors.', 'Two men are on horses that are reared back.', 'Some people who are riding on top of elephants.'], ['Colorful dishes holding meat, vegetables, fruit, and bread.', 'Two giraffes standing in a tree filled area.', 'A white vase filled with different colored flowers.', 'A lone zebra grazing in some green grass.', 'A woman with an umbrella near the sea', 'A small dog is curled up on top of the shoes', 'A pair of horses performing tricks in a field. ', 'there are people riding elephants in the middle of a forest'], ['A bunch of trays that have different food.', 'A giraffe standing next to a forest filled with trees.', 'A vase with red and white flowers outside on a sunny day.', 'a Zebra grazing on grass in a green open field.', 'A girl in a bathing suit with a pink umbrella.', 'a shoe rack with some shoes and a dog sleeping on them', 'Two costumed horse riders make their horses stand on their hind legs.', 'Several elephants in the jungle carrying people on their backs']] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "Iter = iter(data_loader)\n",
    "xdata, ydata = next(Iter) #教師データ、ラベルデータ\n",
    "print(xdata.shape, type(xdata)) \n",
    "print(ydata, type(ydata)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "73aaa1f3-69ad-4ed6-9c9a-519d7f0ed6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n",
      "torch.Size([8, 3, 256, 256])\n",
      "5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/kotomiya/anaconda3/envs/ddgan/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/kotomiya/anaconda3/envs/ddgan/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/kotomiya/anaconda3/envs/ddgan/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/kotomiya/anaconda3/envs/ddgan/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 144, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/kotomiya/anaconda3/envs/ddgan/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 144, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/kotomiya/anaconda3/envs/ddgan/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 140, in collate\n    raise RuntimeError('each element in list of batch should be of equal size')\nRuntimeError: each element in list of batch should be of equal size\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y))\n",
      "File \u001b[0;32m~/anaconda3/envs/ddgan/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/ddgan/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1326\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1325\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n",
      "File \u001b[0;32m~/anaconda3/envs/ddgan/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/ddgan/lib/python3.10/site-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/kotomiya/anaconda3/envs/ddgan/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/kotomiya/anaconda3/envs/ddgan/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/kotomiya/anaconda3/envs/ddgan/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/kotomiya/anaconda3/envs/ddgan/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 144, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/kotomiya/anaconda3/envs/ddgan/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 144, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/kotomiya/anaconda3/envs/ddgan/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 140, in collate\n    raise RuntimeError('each element in list of batch should be of equal size')\nRuntimeError: each element in list of batch should be of equal size\n"
     ]
    }
   ],
   "source": [
    "for iteration, (x, y) in enumerate(data_loader):\n",
    "    print(x.shape)\n",
    "    print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab429719-addf-4297-8cb4-b42350c46294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
